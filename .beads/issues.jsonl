{"id":"lotel-2ie","title":"Remove Docker/compose/spin/signoz runtime paths and retire Docker-specific verification scripts (outside docs)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-13T23:30:53.892456-05:00","updated_at":"2026-02-14T04:56:37.586943301Z","closed_at":"2026-02-14T04:56:37.586943301Z","close_reason":"Closed","labels":["cleanup"],"dependencies":[{"issue_id":"lotel-2ie","depends_on_id":"lotel-3vm","type":"blocks","created_at":"2026-02-13T23:30:58.313882-05:00","created_by":"daemon"}]}
{"id":"lotel-36k","title":"Analyze risks: subprocess lifecycle, stale PID state, metric temporality conversion, and file/version compatibility","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.626672-05:00","updated_at":"2026-02-14T04:40:03.158971457Z","closed_at":"2026-02-14T04:40:03.158971457Z","close_reason":"Closed","labels":["analysis"],"comments":[{"id":5,"issue_id":"lotel-36k","author":"infra-admin","text":"## Risk Analysis: Subprocess Lifecycle, PID State, Metric Temporality, File Compatibility\n\n### 1. Subprocess Lifecycle Risks\n\n**Risk: Orphaned collector process**\n- Scenario: CLI crashes or is killed (SIGKILL) after spawning collector but before recording PID\n- Impact: Collector runs indefinitely; ports locked; next `lotel start` fails\n- Mitigation: Write PID file atomically before spawning (or immediately after fork). On start, check if PID file exists AND process is alive. If PID exists but process is dead, clean up and proceed.\n\n**Risk: Signal forwarding**\n- Scenario: User sends SIGTERM/SIGINT to lotel CLI; collector subprocess must also terminate\n- Impact: If not forwarded, collector becomes orphaned\n- Mitigation: Use `cmd.Process.Signal()` in Go; set up signal handler that forwards to child. Or use process groups (`Setpgid`).\n\n**Risk: Double start**\n- Scenario: User runs `lotel start` twice\n- Impact: Second collector fails on port bind (4317/4318/13133 already in use)\n- Mitigation: Check PID file + process liveness + port availability before starting. Return early with status message if already running.\n\n**Risk: Unclean shutdown leaves ports bound**\n- Scenario: Collector crashes without releasing ports; TIME_WAIT on TCP sockets\n- Impact: Next start fails with \"address already in use\"\n- Mitigation: Check port availability before start; if occupied, check if it's our collector (PID check) or another process.\n\n### 2. Stale PID State Risks\n\n**Risk: PID reuse**\n- Scenario: Collector dies, PID gets reused by unrelated process, `lotel stop` kills wrong process\n- Impact: Kills an innocent process\n- Mitigation: Store PID + process start time (or command line) in state file. On stop, verify the process at PID is actually `otelcol` before sending signals. Example:\n  ```json\n  {\"pid\": 12345, \"started_at\": \"2026-02-14T10:00:00Z\", \"binary\": \"otelcol-contrib\"}\n  ```\n  Check `/proc/{pid}/cmdline` (Linux) or `ps -p {pid} -o comm=` (cross-platform).\n\n**Risk: Stale lock/PID file after crash**\n- Scenario: System crash or power loss; PID file persists but process is gone\n- Impact: `lotel start` refuses to start (\"already running\")\n- Mitigation: Always verify PID liveness, never trust file existence alone. Use `kill(pid, 0)` (signal 0) to check existence without sending a signal.\n\n**Risk: Permission issues on PID file**\n- Scenario: Collector started as different user; PID file not writable\n- Impact: Cannot clean up or stop\n- Mitigation: PID file should live in user-owned directory (`~/.lotel/state/`)\n\n### 3. Metric Temporality Conversion Risks\n\n**Risk: Delta vs Cumulative confusion**\n- Context: OTLP metrics carry `aggregationTemporality` (1=DELTA, 2=CUMULATIVE)\n- Scenario: Application sends cumulative counters; CLI tries to compute rate/delta\n- Impact: Wrong aggregation results (e.g., avg of cumulative values is meaningless without delta conversion)\n- Mitigation:\n  - Store temporality metadata in DuckDB schema\n  - For `avg|min|max` aggregations:\n    - Gauge metrics: aggregate values directly\n    - Cumulative sums: compute deltas between consecutive points before aggregating\n    - Delta sums: aggregate deltas directly\n    - Histograms: sum/count give average; explicit bounds give distribution\n  - Emit structured warnings when mixed temporality is detected\n\n**Risk: Histogram aggregation edge cases**\n- Scenario: User queries `avg` on histogram metric\n- Impact: No single \"value\" to average; must use sum/count\n- Mitigation: For histograms, `avg` = total_sum/total_count over time window. Document this clearly. `min`/`max` are approximations from bucket boundaries.\n\n**Risk: Empty time windows**\n- Scenario: No data points in requested time range\n- Impact: Division by zero in avg, null results\n- Mitigation: Return explicit \"no data\" response with zero count, not an error\n\n**Risk: Clock skew in timestamps**\n- Scenario: Application sends metrics with timestamps significantly different from system clock\n- Impact: Time-range queries miss data\n- Mitigation: Use collector's received timestamp as fallback; document that queries use data point timestamps\n\n### 4. File Format \u0026 Version Compatibility Risks\n\n**Risk: OTLP JSON schema changes**\n- Scenario: otelcol-contrib updates change JSONL output format\n- Impact: Ingestion parser breaks on new fields or changed field names\n- Mitigation: Use lenient JSON parsing; ignore unknown fields; test against multiple collector versions. Pin to a known collector version in recommendations.\n\n**Risk: Parquet schema evolution**\n- Scenario: Adding new columns to DuckDB schema after data already exists\n- Impact: Old Parquet files lack new columns; queries fail or return null\n- Mitigation: DuckDB handles missing columns as NULL automatically with `union_by_name` option. Use this.\n\n**Risk: Concurrent read/write on JSONL files**\n- Scenario: Collector appends to JSONL while ingestion reads it\n- Impact: Partial line read; JSON parse error\n- Mitigation: Ingestion should track file offset (or use inotify). Read only complete lines (ending with \\n). For safety, use file locking or read up to last complete line.\n\n**Risk: Large JSONL files**\n- Scenario: Long-running collector with no ingestion; JSONL grows to GBs\n- Impact: Ingestion takes very long; memory pressure\n- Mitigation: Process JSONL incrementally (streaming). Track last-processed offset in state file. Consider rotating JSONL files periodically (collector file exporter supports rotation in newer versions).\n\n**Risk: DuckDB version incompatibility**\n- Scenario: DuckDB Go driver version changes storage format\n- Impact: Cannot read old data files\n- Mitigation: Use Parquet as interchange format (not DuckDB native files). Parquet is a stable, versioned format.\n\n### Summary of Severity\n| Risk | Severity | Likelihood | Priority |\n|------|----------|------------|----------|\n| Orphaned process | HIGH | MEDIUM | Must-fix |\n| PID reuse (kill wrong process) | CRITICAL | LOW | Must-fix |\n| Stale PID file | MEDIUM | HIGH | Must-fix |\n| Delta/Cumulative confusion | HIGH | HIGH | Must-fix |\n| Empty time window handling | LOW | MEDIUM | Should-fix |\n| Concurrent JSONL read/write | MEDIUM | HIGH | Must-fix |\n| Large JSONL accumulation | MEDIUM | MEDIUM | Should-fix |\n| OTLP schema changes | LOW | LOW | Monitor |\n| DuckDB version compat | LOW | LOW | Mitigated by Parquet |","created_at":"2026-02-14T04:39:58Z"}]}
{"id":"lotel-3kh","title":"Implement traces ingestion from collector disk artifacts into traces partitions with idempotent writes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.567294-05:00","updated_at":"2026-02-14T04:49:27.004598998Z","closed_at":"2026-02-14T04:49:27.004598998Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-3kh","depends_on_id":"lotel-6ir","type":"blocks","created_at":"2026-02-13T23:30:55.409245-05:00","created_by":"daemon"},{"issue_id":"lotel-3kh","depends_on_id":"lotel-731","type":"blocks","created_at":"2026-02-13T23:30:55.496752-05:00","created_by":"daemon"}]}
{"id":"lotel-3vm","title":"Replace Docker-centric verification with Python end-to-end flow (start-\u003eingest-\u003equery-\u003eprune)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:53.705245-05:00","updated_at":"2026-02-14T04:51:58.723962134Z","closed_at":"2026-02-14T04:51:58.723962134Z","close_reason":"Closed","labels":["verify"],"dependencies":[{"issue_id":"lotel-3vm","depends_on_id":"lotel-d2c","type":"blocks","created_at":"2026-02-13T23:30:57.874148-05:00","created_by":"daemon"},{"issue_id":"lotel-3vm","depends_on_id":"lotel-d1j","type":"blocks","created_at":"2026-02-13T23:30:57.959304-05:00","created_by":"daemon"},{"issue_id":"lotel-3vm","depends_on_id":"lotel-y4r","type":"blocks","created_at":"2026-02-13T23:30:58.046304-05:00","created_by":"daemon"},{"issue_id":"lotel-3vm","depends_on_id":"lotel-noy","type":"blocks","created_at":"2026-02-13T23:30:58.134738-05:00","created_by":"daemon"}]}
{"id":"lotel-6ir","title":"Implement DuckDB traces schema + indexes for service/time/trace lookup with deterministic ordering","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.280197-05:00","updated_at":"2026-02-14T04:49:26.722448677Z","closed_at":"2026-02-14T04:49:26.722448677Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-6ir","depends_on_id":"lotel-h9g","type":"blocks","created_at":"2026-02-13T23:30:55.140323-05:00","created_by":"daemon"}]}
{"id":"lotel-731","title":"Analyze collector config semantics, health endpoint behavior, and runtime assumptions","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.334463-05:00","updated_at":"2026-02-14T04:38:03.719532759Z","closed_at":"2026-02-14T04:38:03.719532759Z","close_reason":"Closed","labels":["analysis"],"comments":[{"id":2,"issue_id":"lotel-731","author":"infra-admin","text":"## Analysis: Collector Config Semantics, Health Endpoint, Runtime Assumptions\n\n### Config Files\nTwo configs exist in `docker/configs/otel/`:\n1. **Full config** (`otel-collector-config.yaml`, 137 lines) — production-like\n2. **Minimal config** (`otel-collector-config-minimal.yaml`, 50 lines) — file exports only\n\n### Receivers (What to Preserve for Subprocess)\n**Must keep**:\n- `otlp` receiver: gRPC on `0.0.0.0:4317`, HTTP on `0.0.0.0:4318` — this is the primary ingestion path\n\n**Drop for subprocess model**:\n- `statsd` receiver (port 8125): broken anyway (no Graphite receiver), and StatsD is being deprecated\n- `prometheus` scrape receiver: only scraped `localhost:8889` (self-metrics); unnecessary for file-based model\n\n### Processors (What to Preserve)\n**Must keep**:\n- `batch`: 1s timeout, 1024 send size, 2048 max — standard performance optimization\n- `memory_limiter`: 512 MiB limit, 128 MiB spike — important for subprocess resource control\n\n**Drop or simplify**:\n- `attributes` processor: hardcodes `service.name: canary-api` — this was demo-specific; for real usage, applications set their own service.name\n- `filter` processor: includes only `canary\\.*`, `otel\\.*`, `system\\.*` metrics — too restrictive for general use\n- `resource` processor: hardcodes `host.name: local-dev` — demo-specific\n\n### Exporters (What to Preserve)\n**Must keep**:\n- `file/traces`: path `/data/traces/traces.jsonl`, format json\n- `file/metrics`: path `/data/metrics/metrics.jsonl`, format json\n- `file/logs`: path `/data/logs/logs.jsonl`, format json\n\n**Drop**:\n- `file/traces_json`: duplicate traces output (`traces_detailed.json`)\n- `prometheus` exporter: no Prometheus service in subprocess model\n- `otlp/jaeger` exporter: no Jaeger service in subprocess model\n- `debug` exporter: optional, can be added as verbose flag\n\n### Extensions\n**Must keep**:\n- `health_check` on `0.0.0.0:13133` — critical for readiness checks\n\n**Drop**:\n- `pprof` on `0.0.0.0:1777` — not needed for local dev\n- `zpages` on `0.0.0.0:55679` — not needed for local dev\n\n### Health Endpoint Behavior\n- Endpoint: `http://localhost:13133/`\n- Returns HTTP 200 when collector is ready (all pipelines initialized)\n- Docker health check: `curl -f http://localhost:13133/` every 30s, 10s timeout, 3 retries\n- For subprocess model: poll this endpoint after spawning to confirm readiness\n- The health endpoint is provided by the `health_check` extension, which is standard in otelcol-contrib\n\n### Service Pipelines (Preserve These Semantics)\n```yaml\ntraces:   otlp → [memory_limiter, batch] → file/traces\nmetrics:  otlp → [memory_limiter, batch] → file/metrics\nlogs:     otlp → [memory_limiter, batch] → file/logs\n```\nStripped of demo-specific processors (attributes, filter, resource).\n\n### Runtime Assumptions\n1. **Port availability**: 4317, 4318, 13133 must be free on localhost\n2. **File system**: collector writes JSONL to data directories; files grow unbounded without pruning\n3. **JSONL format**: each line is a complete OTLP JSON object (resourceSpans/resourceMetrics/resourceLogs)\n4. **No rotation**: collector's file exporter appends indefinitely; no built-in rotation\n5. **Crash recovery**: collector restarts cleanly; no WAL or transaction log — data in batch buffer is lost on crash (acceptable for local dev)\n6. **Binary requirement**: subprocess model needs `otelcol-contrib` binary installed on PATH or at a known location\n\n### Recommended Minimal Subprocess Config\n```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc: { endpoint: \"0.0.0.0:4317\" }\n      http: { endpoint: \"0.0.0.0:4318\" }\nprocessors:\n  batch:\n    timeout: 1s\n    send_batch_size: 1024\n    send_batch_max_size: 2048\nexporters:\n  file/traces:\n    path: ${DATA_DIR}/traces/traces.jsonl\n    format: json\n  file/metrics:\n    path: ${DATA_DIR}/metrics/metrics.jsonl\n    format: json\n  file/logs:\n    path: ${DATA_DIR}/logs/logs.jsonl\n    format: json\nextensions:\n  health_check:\n    endpoint: 0.0.0.0:13133\nservice:\n  extensions: [health_check]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [file/traces]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [file/metrics]\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [file/logs]\n```","created_at":"2026-02-14T04:37:58Z"}]}
{"id":"lotel-7r3","title":"Implement metrics avg|min|max aggregation with standardized temporality conversion rules and structured warnings","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:53.227888-05:00","updated_at":"2026-02-14T04:49:32.520069972Z","closed_at":"2026-02-14T04:49:32.520069972Z","close_reason":"Closed","labels":["query"],"dependencies":[{"issue_id":"lotel-7r3","depends_on_id":"lotel-9ho","type":"blocks","created_at":"2026-02-13T23:30:56.65091-05:00","created_by":"daemon"},{"issue_id":"lotel-7r3","depends_on_id":"lotel-36k","type":"blocks","created_at":"2026-02-13T23:30:56.738691-05:00","created_by":"daemon"}]}
{"id":"lotel-84l","title":"Implement prune planner with --older-than (hours/days) and dry-run reporting by partition/file/bytes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:53.327205-05:00","updated_at":"2026-02-14T04:49:37.077571073Z","closed_at":"2026-02-14T04:49:37.077571073Z","close_reason":"Closed","labels":["prune"],"dependencies":[{"issue_id":"lotel-84l","depends_on_id":"lotel-h9g","type":"blocks","created_at":"2026-02-13T23:30:56.825422-05:00","created_by":"daemon"},{"issue_id":"lotel-84l","depends_on_id":"lotel-36k","type":"blocks","created_at":"2026-02-13T23:30:56.916118-05:00","created_by":"daemon"}]}
{"id":"lotel-87z","title":"Implement metrics ingestion into metrics partitions preserving temporality and conversion metadata","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.661227-05:00","updated_at":"2026-02-14T04:49:27.098972646Z","closed_at":"2026-02-14T04:49:27.098972646Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-87z","depends_on_id":"lotel-k6x","type":"blocks","created_at":"2026-02-13T23:30:55.585372-05:00","created_by":"daemon"},{"issue_id":"lotel-87z","depends_on_id":"lotel-731","type":"blocks","created_at":"2026-02-13T23:30:55.681444-05:00","created_by":"daemon"}]}
{"id":"lotel-89q","title":"Implement DuckDB logs schema + indexes for service/time querying with structured attribute support","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.474447-05:00","updated_at":"2026-02-14T04:49:26.909565497Z","closed_at":"2026-02-14T04:49:26.909565497Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-89q","depends_on_id":"lotel-h9g","type":"blocks","created_at":"2026-02-13T23:30:55.317096-05:00","created_by":"daemon"}]}
{"id":"lotel-8uv","title":"Implement stale lock/PID recovery and restart-safe behavior for interrupted collector sessions","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.088694-05:00","updated_at":"2026-02-14T04:49:20.059909678Z","closed_at":"2026-02-14T04:49:20.059909678Z","close_reason":"Closed","labels":["process"],"dependencies":[{"issue_id":"lotel-8uv","depends_on_id":"lotel-ds0","type":"blocks","created_at":"2026-02-13T23:30:54.78717-05:00","created_by":"daemon"},{"issue_id":"lotel-8uv","depends_on_id":"lotel-36k","type":"blocks","created_at":"2026-02-13T23:30:54.877974-05:00","created_by":"daemon"}]}
{"id":"lotel-9ho","title":"Implement metrics query command with service.name filtering and absolute/relative time windows","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:53.040131-05:00","updated_at":"2026-02-14T04:49:32.332426509Z","closed_at":"2026-02-14T04:49:32.332426509Z","close_reason":"Closed","labels":["query"],"dependencies":[{"issue_id":"lotel-9ho","depends_on_id":"lotel-ni1","type":"blocks","created_at":"2026-02-13T23:30:56.299592-05:00","created_by":"daemon"},{"issue_id":"lotel-9ho","depends_on_id":"lotel-87z","type":"blocks","created_at":"2026-02-13T23:30:56.388116-05:00","created_by":"daemon"}]}
{"id":"lotel-9km","title":"Analyze current lotel CLI command structure and Docker-coupled start/stop behavior","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.240598-05:00","updated_at":"2026-02-14T04:37:27.996936385Z","closed_at":"2026-02-14T04:37:27.996936385Z","close_reason":"Closed","labels":["analysis"],"comments":[{"id":1,"issue_id":"lotel-9km","author":"infra-admin","text":"## Analysis: Current CLI Command Structure \u0026 Docker-Coupled Start/Stop\n\n### CLI Entrypoint\n- **File**: `cmd/lotel/main.go` (61 lines)\n- **Framework**: Cobra CLI + Charmbracelet Fang for execution\n- **Commands**: Only `start` and `stop` exist today\n\n### Command Flow\n\n**`lotel start`**:\n1. Calls `config.ResolvePaths()` to get config file path + data directory\n2. Creates Docker API client via `docker.NewClient()`\n3. Calls `dc.StartCollector(ctx, configPath, dataPath)`\n4. StartCollector: pulls image → checks for existing container → ensures network → creates container with port bindings and volume mounts → starts container\n\n**`lotel stop`**:\n1. Creates Docker API client\n2. Calls `dc.StopCollector(ctx)`\n3. StopCollector: lists all containers → finds by name → stops (10s timeout) → removes container\n\n### Docker Coupling Points\n- `internal/docker/docker.go` (173 lines): All orchestration lives here\n  - Uses `github.com/docker/docker` client SDK directly\n  - Creates `telemetry-nest-network` bridge network\n  - Manages container named `telemetry-nest-otel-collector`\n  - Pulls `otel/opentelemetry-collector-contrib:latest`\n  - Binds ports 4317, 4318, 13133, 8889\n  - Mounts config at `/etc/otel-collector-config.yaml`\n  - Mounts data at `/data` from `~/.lotel/data`\n  - Sets health check: `curl -f http://localhost:13133/`\n  - Restart policy: `unless-stopped`\n\n- `internal/config/config.go` (41 lines): Path resolution and constants\n  - `ImageName = \"otel/opentelemetry-collector-contrib:latest\"`\n  - `ContainerName = \"telemetry-nest-otel-collector\"`\n  - `NetworkName = \"telemetry-nest-network\"`\n  - Config path: `{cwd}/docker/configs/otel/otel-collector-config.yaml`\n  - Data path: `~/.lotel/data/{traces,metrics,logs}`\n\n### Dependencies (go.mod)\n- `github.com/docker/docker v28.5.2` — Docker API client\n- `github.com/docker/go-connections v0.6.0` — NAT port bindings\n- These are the ONLY Docker-specific deps; removing them is straightforward\n\n### Migration Path to Subprocess Model\n1. Replace `internal/docker/` with `internal/process/` or `internal/collector/`\n2. Use `os/exec` to spawn `otelcol-contrib` binary as subprocess\n3. Store PID in `~/.lotel/state/collector.pid` for lifecycle management\n4. Config file can remain at same path or move to `~/.lotel/config/`\n5. Remove Docker deps from go.mod\n6. Add `status` command (not present today)\n7. Health check should use same HTTP endpoint: `http://localhost:13133/`\n\n### No Other Commands Exist\n- No `query`, `prune`, `status`, `health` commands today\n- The CLI is a thin wrapper around Docker container lifecycle\n- No state management beyond container existence","created_at":"2026-02-14T04:37:24Z"}]}
{"id":"lotel-as8","title":"Implement health/readiness checks using collector health endpoint; fail readiness unless endpoint passes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.89968-05:00","updated_at":"2026-02-14T04:49:19.921400522Z","closed_at":"2026-02-14T04:49:19.921400522Z","close_reason":"Closed","labels":["process"],"dependencies":[{"issue_id":"lotel-as8","depends_on_id":"lotel-731","type":"blocks","created_at":"2026-02-13T23:30:54.526948-05:00","created_by":"daemon"},{"issue_id":"lotel-as8","depends_on_id":"lotel-ds0","type":"blocks","created_at":"2026-02-13T23:30:54.613285-05:00","created_by":"daemon"}]}
{"id":"lotel-b4x","title":"Add Go/Python tests for subprocess health, query determinism, metrics aggregation edge cases, and prune safety","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:53.795391-05:00","updated_at":"2026-02-14T04:51:58.792229512Z","closed_at":"2026-02-14T04:51:58.792229512Z","close_reason":"Closed","labels":["verify"],"dependencies":[{"issue_id":"lotel-b4x","depends_on_id":"lotel-3vm","type":"blocks","created_at":"2026-02-13T23:30:58.223213-05:00","created_by":"daemon"}]}
{"id":"lotel-bt2","title":"Post-migration cleanup/polish: remove dead code paths, tighten errors, and improve operator messages","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-13T23:30:54.075518-05:00","updated_at":"2026-02-14T04:56:37.735610626Z","closed_at":"2026-02-14T04:56:37.735610626Z","close_reason":"Closed","labels":["cleanup"],"dependencies":[{"issue_id":"lotel-bt2","depends_on_id":"lotel-2ie","type":"blocks","created_at":"2026-02-13T23:30:58.491687-05:00","created_by":"daemon"},{"issue_id":"lotel-bt2","depends_on_id":"lotel-b4x","type":"blocks","created_at":"2026-02-13T23:30:58.575934-05:00","created_by":"daemon"}]}
{"id":"lotel-c0w","title":"Add clear collector runtime entrypoint under cmd while keeping lotel CLI coherent for coding-agent usage","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:51.994439-05:00","updated_at":"2026-02-14T04:43:21.724414667Z","closed_at":"2026-02-14T04:43:21.724414667Z","close_reason":"Closed","labels":["process"],"dependencies":[{"issue_id":"lotel-c0w","depends_on_id":"lotel-9km","type":"blocks","created_at":"2026-02-13T23:30:54.699862-05:00","created_by":"daemon"}]}
{"id":"lotel-d1j","title":"Add verification for direct OTLP ingestion (trace+metric+log) using UUID service.name and query assertions","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:53.518504-05:00","updated_at":"2026-02-14T04:51:58.586866895Z","closed_at":"2026-02-14T04:51:58.586866895Z","close_reason":"Closed","labels":["verify"],"dependencies":[{"issue_id":"lotel-d1j","depends_on_id":"lotel-as8","type":"blocks","created_at":"2026-02-13T23:30:57.084787-05:00","created_by":"daemon"},{"issue_id":"lotel-d1j","depends_on_id":"lotel-drj","type":"blocks","created_at":"2026-02-13T23:30:57.171202-05:00","created_by":"daemon"},{"issue_id":"lotel-d1j","depends_on_id":"lotel-9ho","type":"blocks","created_at":"2026-02-13T23:30:57.256796-05:00","created_by":"daemon"},{"issue_id":"lotel-d1j","depends_on_id":"lotel-j91","type":"blocks","created_at":"2026-02-13T23:30:57.347507-05:00","created_by":"daemon"},{"issue_id":"lotel-d1j","depends_on_id":"lotel-7r3","type":"blocks","created_at":"2026-02-13T23:30:57.435403-05:00","created_by":"daemon"}]}
{"id":"lotel-d2c","title":"Analyze Docker-era verification/test assets and define replacement scope for Python end-to-end flow","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.523325-05:00","updated_at":"2026-02-14T04:39:15.211437195Z","closed_at":"2026-02-14T04:39:15.211437195Z","close_reason":"Closed","labels":["analysis"],"comments":[{"id":4,"issue_id":"lotel-d2c","author":"infra-admin","text":"## Analysis: Docker-Era Verification/Test Assets \u0026 Replacement Scope\n\n### Current Verification Assets\n\n**1. Bash health check** (`scripts/verification/bash/check_telemetry_health.sh`, 241 lines)\n- Checks 6 Docker containers running (otel-collector, statsd, prometheus, grafana, jaeger, filebeat)\n- Checks service endpoints: OTel health, Prometheus, Grafana, Jaeger, StatsD admin\n- Checks data directories exist and are writable\n- Checks container-to-container network connectivity (docker exec nc)\n- Checks config files exist\n- Checks for recent telemetry data files\n- Queries Prometheus for active targets\n- **Docker dependencies**: docker ps, docker exec, docker-compose\n- **Verdict**: REPLACE entirely — subprocess model has no containers to check\n\n**2. Python pipeline test** (`scripts/verification/python/test_metrics_pipeline.py`, 409 lines)\n- `check_services_health()`: HTTP checks on OTel, Prometheus, Grafana, Jaeger\n- `send_statsd_metrics()`: UDP to localhost:8125 (broken)\n- `send_otlp_traces()`: HTTP POST to localhost:4318/v1/traces\n- `send_otlp_metrics()`: HTTP POST to localhost:4318/v1/metrics\n- `verify_file_exports()`: checks data/ files exist and have content\n- `check_prometheus_metrics()`: queries Prometheus API\n- **Docker dependencies**: assumes Docker stack is running, checks Prometheus/Grafana/Jaeger\n- **Reusable parts**: OTLP trace/metric submission code structure\n- **Verdict**: REPLACE — keep OTLP submission patterns but rewrite verification against CLI queries\n\n**3. Go trace generator** (`scripts/verification/go/trace_generator.go`, 293 lines)\n- Generates realistic Canary API traces with random IDs, parent-child relationships\n- Sends via HTTP POST to localhost:4318/v1/traces\n- **Docker dependencies**: none (pure OTLP HTTP)\n- **Verdict**: KEEP as optional smoke-test tool; not critical path\n\n**4. Shell setup/start/stop scripts** (`scripts/setup/`, ~630 lines total)\n- `setup-telemetry-env.sh`: Docker prereqs, network, volumes, image pulls\n- `start-telemetry-stack.sh`: docker-compose up with dependency ordering\n- `stop-telemetry-stack.sh`: docker-compose down with cleanup\n- **Verdict**: REMOVE — replaced by `lotel start/stop`\n\n**5. Example apps** (`examples/python-fastapi/`, `examples/distributed/`)\n- Self-contained Docker Compose apps that send telemetry\n- Not verification per se, but reference implementations\n- **Verdict**: KEEP in docs/examples — they work against any OTLP endpoint\n\n### Replacement: Python End-to-End Verification Flow\n\n**New script**: `scripts/verify.py` (or `tests/e2e/test_verify.py`)\n\n**Flow**:\n1. **Check collector**: `lotel status` returns healthy (or `curl http://localhost:13133/`)\n2. **Generate UUID service tag**: `test-verify-{uuid4()}`\n3. **Send OTLP traces**: POST to localhost:4318/v1/traces with UUID service.name\n4. **Send OTLP metrics**: POST to localhost:4318/v1/metrics with UUID service.name\n5. **Send OTLP logs**: POST to localhost:4318/v1/logs with UUID service.name\n6. **Wait for ingestion**: sleep or poll\n7. **Query traces**: `lotel query traces --service {uuid}` → assert non-empty JSON\n8. **Query metrics**: `lotel query metrics --service {uuid}` → assert non-empty JSON\n9. **Query logs**: `lotel query logs --service {uuid}` → assert non-empty JSON\n10. **Test prune**: `lotel prune --older-than 0h --service {uuid} --dry-run` → assert lists files\n11. **Execute prune**: `lotel prune --older-than 0h --service {uuid}` → assert deletion\n12. **Verify deletion**: `lotel query traces --service {uuid}` → assert empty\n\n**Dependencies**: Only `requests` (for OTLP HTTP) + `subprocess` (for CLI commands)\n**No Docker dependency at all.**\n\n### Assets to Remove\n- `scripts/verification/bash/check_telemetry_health.sh`\n- `scripts/verification/python/test_metrics_pipeline.py`\n- `scripts/setup/setup-telemetry-env.sh`\n- `scripts/setup/start-telemetry-stack.sh`\n- `scripts/setup/stop-telemetry-stack.sh`\n\n### Assets to Keep\n- `scripts/verification/go/trace_generator.go` — optional, no Docker dep\n- `examples/` — reference apps, not verification\n- `docs/` — reference material","created_at":"2026-02-14T04:39:11Z"}]}
{"id":"lotel-da3","title":"Finalize DuckDB embedding plan (driver/build constraints, schema boundaries, and migration notes)","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.717552-05:00","updated_at":"2026-02-14T04:44:24.539755936Z","closed_at":"2026-02-14T04:44:24.539755936Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-da3","depends_on_id":"lotel-iic","type":"blocks","created_at":"2026-02-13T23:30:54.165431-05:00","created_by":"daemon"},{"issue_id":"lotel-da3","depends_on_id":"lotel-36k","type":"blocks","created_at":"2026-02-13T23:30:54.264195-05:00","created_by":"daemon"}],"comments":[{"id":6,"issue_id":"lotel-da3","author":"infra-admin","text":"## DuckDB Embedding Plan\n\n### Driver Selection\n- **Package**: github.com/duckdb/duckdb-go/v2 (official, maintained by DuckDB team)\n- **Interface**: standard database/sql — no vendor lock-in\n- **Build**: uses CGO (C library embedded); cross-compiles via standard Go toolchain\n- **Version**: latest duckdb-go v2.5.5+ (DuckDB v1.4.4)\n\n### Build Constraints\n- CGO_ENABLED=1 required (default on most systems)\n- Linux amd64/arm64: works out of the box\n- macOS amd64/arm64: works out of the box\n- Windows: not a target (per constraints)\n- Binary size increase: ~30-50MB due to embedded DuckDB C library\n\n### Architecture Decision: In-Process DuckDB (not Parquet files)\nAfter analysis, the simpler approach is better:\n- Use a single DuckDB database file at ~/.lotel/data/lotel.db\n- DuckDB supports concurrent reads from multiple processes\n- Writes are serialized via DuckDB's internal WAL\n- No need for a separate Parquet ingestion pipeline\n- DuckDB can directly query JSONL via read_json() for initial import\n\n### Schema Boundaries\nThree tables, one per signal:\n\n**traces**: spans extracted from OTLP resourceSpans JSONL\n**metrics**: data points extracted from OTLP resourceMetrics JSONL\n**logs**: log records extracted from OTLP resourceLogs JSONL\n\nEach table partitioned by (service_name, date) using DuckDB partition columns.\nThis gives efficient pruning: DELETE FROM traces WHERE service_name = ? AND date \u003c ?\n\n### Migration Notes\n- Phase 1: JSONL ingestion into DuckDB on query (lazy load)\n- Phase 2: Background ingestion after start (eager load)\n- No schema migration framework needed initially — CREATE TABLE IF NOT EXISTS\n- If schema changes, drop and re-ingest from JSONL source files\n\n### Why Not Parquet Files\n- Adds ingestion pipeline complexity\n- DuckDB can query JSONL directly for small-to-medium datasets\n- For local dev telemetry volumes, DuckDB in-process is more than fast enough\n- Simpler pruning via SQL DELETE vs directory tree management","created_at":"2026-02-14T04:44:19Z"}]}
{"id":"lotel-drj","title":"Implement traces query command with service.name first-class filtering and absolute/relative time windows","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.943444-05:00","updated_at":"2026-02-14T04:49:32.239242163Z","closed_at":"2026-02-14T04:49:32.239242163Z","close_reason":"Closed","labels":["query"],"dependencies":[{"issue_id":"lotel-drj","depends_on_id":"lotel-ni1","type":"blocks","created_at":"2026-02-13T23:30:56.114037-05:00","created_by":"daemon"},{"issue_id":"lotel-drj","depends_on_id":"lotel-3kh","type":"blocks","created_at":"2026-02-13T23:30:56.202486-05:00","created_by":"daemon"}]}
{"id":"lotel-ds0","title":"Implement collector subprocess lifecycle in CLI (start/stop/status) with crash-safe PID/state handling","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.805352-05:00","updated_at":"2026-02-14T04:43:21.655792615Z","closed_at":"2026-02-14T04:43:21.655792615Z","close_reason":"Closed","labels":["process"],"dependencies":[{"issue_id":"lotel-ds0","depends_on_id":"lotel-9km","type":"blocks","created_at":"2026-02-13T23:30:54.350854-05:00","created_by":"daemon"},{"issue_id":"lotel-ds0","depends_on_id":"lotel-731","type":"blocks","created_at":"2026-02-13T23:30:54.441523-05:00","created_by":"daemon"}]}
{"id":"lotel-h9g","title":"Implement signal/service/date partition manager at $HOME/.lotel/data/\u003csignal\u003e/service.name=\u003csvc\u003e/YYYY/MM/DD","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.183035-05:00","updated_at":"2026-02-14T04:49:26.610301377Z","closed_at":"2026-02-14T04:49:26.610301377Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-h9g","depends_on_id":"lotel-iic","type":"blocks","created_at":"2026-02-13T23:30:54.965532-05:00","created_by":"daemon"},{"issue_id":"lotel-h9g","depends_on_id":"lotel-da3","type":"blocks","created_at":"2026-02-13T23:30:55.051143-05:00","created_by":"daemon"}]}
{"id":"lotel-iic","title":"Analyze current telemetry disk outputs and define DuckDB-backed signal/service/date partition contract under $HOME/.lotel/data","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:51.42882-05:00","updated_at":"2026-02-14T04:38:41.754099621Z","closed_at":"2026-02-14T04:38:41.754099621Z","close_reason":"Closed","labels":["analysis"],"comments":[{"id":3,"issue_id":"lotel-iic","author":"infra-admin","text":"## Analysis: Telemetry Disk Outputs \u0026 DuckDB Partition Contract\n\n### Current Disk Output Format\n\n**Location**: `~/.lotel/data/{traces,metrics,logs}/`\n\n**File format**: JSONL (JSON Lines) — one JSON object per line\n- `traces.jsonl`: Each line is an OTLP `resourceSpans` export batch\n- `metrics.jsonl`: Each line is an OTLP `resourceMetrics` export batch\n- `logs.jsonl`: Each line is an OTLP `resourceLogs` export batch\n\n**Sample OTLP trace line structure**:\n```json\n{\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"service.name\",\"value\":{\"stringValue\":\"my-svc\"}}]},\"scopeSpans\":[{\"spans\":[{\"traceId\":\"...\",\"spanId\":\"...\",\"name\":\"GET /foo\",\"startTimeUnixNano\":\"...\",\"endTimeUnixNano\":\"...\",\"attributes\":[...]}]}]}]}\n```\n\n**Current issues with flat JSONL**:\n1. Single file per signal — grows unbounded\n2. No partitioning by service or date — full scan for any query\n3. No indexing — O(n) for any lookup\n4. Mixed services in same file — can't prune per-service\n5. Append-only from collector — no rotation\n\n### Proposed DuckDB Partition Contract\n\n**Root**: `$HOME/.lotel/data/`\n\n**Physical layout** (Hive-style partitioning):\n```\n$HOME/.lotel/data/\n├── traces/\n│   └── service.name=my-svc/\n│       └── 2026/02/14/\n│           └── data.parquet    (or DuckDB page file)\n├── metrics/\n│   └── service.name=my-svc/\n│       └── 2026/02/14/\n│           └── data.parquet\n└── logs/\n    └── service.name=my-svc/\n        └── 2026/02/14/\n            └── data.parquet\n```\n\n**Why this layout**:\n- Service-first: enables per-service pruning and queries (most common filter)\n- Date partitioned: enables time-range queries and age-based pruning\n- Compatible with DuckDB's Hive partitioning reads\n- Human-navigable for debugging\n\n### DuckDB Approach Options\n\n**Option A: DuckDB as embedded query engine over Parquet files**\n- Ingestion: parse JSONL → write Parquet files partitioned by service+date\n- Query: DuckDB reads Parquet files using Hive partitioning\n- Pros: Files are portable, no DB lock contention, can prune by deleting directories\n- Cons: Need a separate ingestion step; Parquet write adds complexity\n\n**Option B: DuckDB as single database file**\n- Single `~/.lotel/data/lotel.duckdb` file\n- Pros: Simpler writes, atomic transactions, built-in indexing\n- Cons: File locking (only one process can write), harder to prune, less transparent\n\n**Recommended: Option A (Parquet + DuckDB query engine)**\nReasons:\n- Collector subprocess writes JSONL (no change needed to collector)\n- A background ingestion routine or CLI command parses JSONL → Parquet\n- DuckDB queries Parquet files at read time with partition pruning\n- Pruning = `rm -rf` on date directories (simple, safe, atomic per-directory)\n- No lock contention between collector writes and CLI queries\n\n### Ingestion Pipeline\n```\nCollector → ~/.lotel/data/raw/{traces,metrics,logs}.jsonl  (append-only JSONL)\n                    ↓ (lotel ingest, or triggered by query)\n            ~/.lotel/data/{signal}/service.name={svc}/YYYY/MM/DD/data.parquet\n                    ↓ (lotel query)\n            DuckDB reads Parquet with Hive partition pruning\n```\n\n### Schema Contracts\n\n**Traces table** (per partition file):\n| Column | Type | Source |\n|--------|------|--------|\n| trace_id | VARCHAR | span.traceId |\n| span_id | VARCHAR | span.spanId |\n| parent_span_id | VARCHAR | span.parentSpanId |\n| name | VARCHAR | span.name |\n| kind | INTEGER | span.kind |\n| start_time | TIMESTAMP | span.startTimeUnixNano |\n| end_time | TIMESTAMP | span.endTimeUnixNano |\n| duration_ns | BIGINT | computed |\n| status_code | INTEGER | span.status.code |\n| service_name | VARCHAR | resource.attributes[service.name] |\n| attributes | JSON | span.attributes (flattened) |\n\n**Metrics table**:\n| Column | Type | Source |\n|--------|------|--------|\n| metric_name | VARCHAR | metric.name |\n| metric_type | VARCHAR | sum/gauge/histogram/summary |\n| value | DOUBLE | dataPoint value |\n| timestamp | TIMESTAMP | dataPoint.timeUnixNano |\n| service_name | VARCHAR | resource.attributes[service.name] |\n| aggregation_temporality | INTEGER | 1=delta, 2=cumulative |\n| is_monotonic | BOOLEAN | for sums |\n| attributes | JSON | dataPoint.attributes |\n\n**Logs table**:\n| Column | Type | Source |\n|--------|------|--------|\n| timestamp | TIMESTAMP | logRecord.timeUnixNano |\n| severity | VARCHAR | logRecord.severityText |\n| severity_number | INTEGER | logRecord.severityNumber |\n| body | VARCHAR | logRecord.body.stringValue |\n| service_name | VARCHAR | resource.attributes[service.name] |\n| trace_id | VARCHAR | logRecord.traceId |\n| span_id | VARCHAR | logRecord.spanId |\n| attributes | JSON | logRecord.attributes |\n\n### Pruning Contract\n- `lotel prune --older-than 7d` → deletes all partition directories older than 7 days\n- `lotel prune --older-than 24h --service my-svc` → scoped to one service\n- Dry-run mode: list directories/files/bytes that would be deleted\n- No DuckDB involvement for pruning — pure filesystem operations on partition directories","created_at":"2026-02-14T04:38:37Z"}]}
{"id":"lotel-j91","title":"Implement logs query command with service.name filtering and absolute/relative time windows","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:53.134524-05:00","updated_at":"2026-02-14T04:49:32.426529886Z","closed_at":"2026-02-14T04:49:32.426529886Z","close_reason":"Closed","labels":["query"],"dependencies":[{"issue_id":"lotel-j91","depends_on_id":"lotel-ni1","type":"blocks","created_at":"2026-02-13T23:30:56.474098-05:00","created_by":"daemon"},{"issue_id":"lotel-j91","depends_on_id":"lotel-po6","type":"blocks","created_at":"2026-02-13T23:30:56.564702-05:00","created_by":"daemon"}]}
{"id":"lotel-k6x","title":"Implement DuckDB metrics schema + indexes including temporality/type fields required for window aggregation","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.381012-05:00","updated_at":"2026-02-14T04:49:26.815917609Z","closed_at":"2026-02-14T04:49:26.815917609Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-k6x","depends_on_id":"lotel-h9g","type":"blocks","created_at":"2026-02-13T23:30:55.228173-05:00","created_by":"daemon"}]}
{"id":"lotel-ni1","title":"Define and implement machine-readable query contract (JSON default, stable ordering, explicit exit codes, --output human optional)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.8509-05:00","updated_at":"2026-02-14T04:49:19.990897585Z","closed_at":"2026-02-14T04:49:19.990897585Z","close_reason":"Closed","labels":["query"],"dependencies":[{"issue_id":"lotel-ni1","depends_on_id":"lotel-9km","type":"blocks","created_at":"2026-02-13T23:30:55.936981-05:00","created_by":"daemon"},{"issue_id":"lotel-ni1","depends_on_id":"lotel-iic","type":"blocks","created_at":"2026-02-13T23:30:56.026226-05:00","created_by":"daemon"}]}
{"id":"lotel-noy","title":"Implement safe prune execution and reporting for telemetry partitions older than age threshold","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:53.424013-05:00","updated_at":"2026-02-14T04:49:37.171255916Z","closed_at":"2026-02-14T04:49:37.171255916Z","close_reason":"Closed","labels":["prune"],"dependencies":[{"issue_id":"lotel-noy","depends_on_id":"lotel-84l","type":"blocks","created_at":"2026-02-13T23:30:57.002384-05:00","created_by":"daemon"}]}
{"id":"lotel-pfs","title":"Update docs for local collector workflow (start/stop/query/prune), JSON default output, and DuckDB troubleshooting","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-13T23:30:53.984793-05:00","updated_at":"2026-02-14T04:56:37.665977418Z","closed_at":"2026-02-14T04:56:37.665977418Z","close_reason":"Closed","labels":["docs"],"dependencies":[{"issue_id":"lotel-pfs","depends_on_id":"lotel-3vm","type":"blocks","created_at":"2026-02-13T23:30:58.405546-05:00","created_by":"daemon"}]}
{"id":"lotel-po6","title":"Implement logs ingestion into logs partitions preserving timestamp/severity/resource attributes","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-13T23:30:52.755127-05:00","updated_at":"2026-02-14T04:49:27.193088343Z","closed_at":"2026-02-14T04:49:27.193088343Z","close_reason":"Closed","labels":["storage"],"dependencies":[{"issue_id":"lotel-po6","depends_on_id":"lotel-89q","type":"blocks","created_at":"2026-02-13T23:30:55.764422-05:00","created_by":"daemon"},{"issue_id":"lotel-po6","depends_on_id":"lotel-731","type":"blocks","created_at":"2026-02-13T23:30:55.851036-05:00","created_by":"daemon"}]}
{"id":"lotel-y4r","title":"Add telemetrygen ingestion smoke validation for traces/metrics/logs as blocking verification","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-13T23:30:53.613292-05:00","updated_at":"2026-02-14T04:51:58.655396546Z","closed_at":"2026-02-14T04:51:58.655396546Z","close_reason":"Closed","labels":["verify"],"dependencies":[{"issue_id":"lotel-y4r","depends_on_id":"lotel-as8","type":"blocks","created_at":"2026-02-13T23:30:57.522044-05:00","created_by":"daemon"},{"issue_id":"lotel-y4r","depends_on_id":"lotel-drj","type":"blocks","created_at":"2026-02-13T23:30:57.608184-05:00","created_by":"daemon"},{"issue_id":"lotel-y4r","depends_on_id":"lotel-9ho","type":"blocks","created_at":"2026-02-13T23:30:57.695349-05:00","created_by":"daemon"},{"issue_id":"lotel-y4r","depends_on_id":"lotel-j91","type":"blocks","created_at":"2026-02-13T23:30:57.783172-05:00","created_by":"daemon"}]}
